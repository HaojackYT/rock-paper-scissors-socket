from fastapi import FastAPI, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
import cv2
import mediapipe as mp
import numpy as np
import asyncio
from concurrent.futures import ThreadPoolExecutor
import uvicorn
import os

# ·∫®n warning TensorFlow/MediaPipe
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

# ========== C·∫§U H√åNH ==========
app = FastAPI(title="‚úã API Nh·∫≠n di·ªán K√©o B√∫a Bao")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Cho ph√©p m·ªçi ngu·ªìn truy c·∫≠p
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Kh·ªüi t·∫°o MediaPipe 1 l·∫ßn (kh√¥ng t·∫°o l·∫°i m·ªói l·∫ßn request)
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

# Thread pool cho x·ª≠ l√Ω song song (ƒëa lu·ªìng)
executor = ThreadPoolExecutor(max_workers=4)  # C√≥ th·ªÉ ch·ªânh t√πy CPU


# ========== H√ÄM X·ª¨ L√ù ==========
def get_finger_status(hand_landmarks):
    fingers = []
    lm = hand_landmarks.landmark

    # Ng√≥n c√°i (theo tr·ª•c x)
    if lm[4].x < lm[3].x:
        fingers.append(1)
    else:
        fingers.append(0)

    # C√°c ng√≥n c√≤n l·∫°i (theo tr·ª•c y)
    for tip, pip in zip([8, 12, 16, 20], [6, 10, 14, 18]):
        fingers.append(1 if lm[tip].y < lm[pip].y else 0)

    return fingers


def classify_gesture(fingers):
    total = sum(fingers)
    if total == 0:
        return "B√öA"
    elif total == 2 and fingers[1] == 1 and fingers[2] == 1:
        return "K√âO"
    elif total == 5:
        return "BAO"
    else:
        return "KH√îNG R√ï"


def process_image(image: np.ndarray) -> str:
    """X·ª≠ l√Ω ·∫£nh trong lu·ªìng ri√™ng"""
    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # M·ªói lu·ªìng t·∫°o 1 instance ri√™ng (thread-safe)
    with mp_hands.Hands(static_image_mode=True, max_num_hands=1,
                        min_detection_confidence=0.7) as hands:
        result = hands.process(rgb)

    gesture = "Kh√¥ng th·∫•y tay"
    if result.multi_hand_landmarks:
        for hand_landmarks in result.multi_hand_landmarks:
            fingers = get_finger_status(hand_landmarks)
            gesture = classify_gesture(fingers)

    return gesture


# ========== API ENDPOINT ==========
@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    """X·ª≠ l√Ω ·∫£nh nh·∫≠n di·ªán tay - ch·∫°y b·∫•t ƒë·ªìng b·ªô + ƒëa lu·ªìng"""
    contents = await file.read()
    np_arr = np.frombuffer(contents, np.uint8)
    image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)

    if image is None:
        return {"error": "Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh"}

    loop = asyncio.get_event_loop()
    gesture = await loop.run_in_executor(executor, process_image, image)

    return {"gesture": gesture}


# ========== CH·∫†Y SERVER ==========
if __name__ == "__main__":
    print("üöÄ Server ƒëang ch·∫°y t·∫°i: http://127.0.0.1:8000")
    # Ch·∫°y ƒëa ti·∫øn tr√¨nh (4 worker) ‚Äî m·ªói worker c√≥ ThreadPoolExecutor ri√™ng
    uvicorn.run("main:app", host="0.0.0.0", port=8000, workers=4)